#!/usr/bin/env python3
# script: grade_assessment.py

import json
import math
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging

class RiskLevel(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø±"""
    LOW = "Ù…Ù†Ø®ÙØ¶"
    MEDIUM = "Ù…ØªÙˆØ³Ø·"
    HIGH = "Ø¹Ø§Ù„ÙŠ"
    CRITICAL = "Ø­Ø±Ø¬"

class QualityMetric(Enum):
    """Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©"""
    ARCHITECTURE = "architecture"
    CODE_QUALITY = "code_quality"
    SECURITY = "security"
    SCALABILITY = "scalability"
    MAINTAINABILITY = "maintainability"
    TESTING = "testing"
    DOCUMENTATION = "documentation"
    PERFORMANCE = "performance"

@dataclass
class ScoreCard:
    """Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
    metric: str
    score: float  # 0-10
    weight: float  # 0-1
    evidence: List[str]
    recommendations: List[str]

    @property
    def weighted_score(self) -> float:
        return self.score * self.weight

@dataclass
class RiskItem:
    """Ø¹Ù†ØµØ± Ù…Ø®Ø§Ø·Ø±Ø©"""
    id: str
    title: str
    description: str
    category: str
    probability: float  # 0-1
    impact: float  # 0-10
    level: RiskLevel
    mitigation_strategies: List[str]

    @property
    def risk_score(self) -> float:
        return self.probability * self.impact

class QualityAssessor:
    """Ù…Ù‚ÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""

    def __init__(self, repo_path: str, output_dir: str):
        self.repo_path = Path(repo_path)
        self.output_dir = Path(output_dir)
        self.logger = logging.getLogger(__name__)

        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± (ÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµÙ‡Ø§)
        self.weights = {
            QualityMetric.ARCHITECTURE: 0.20,
            QualityMetric.CODE_QUALITY: 0.15,
            QualityMetric.SECURITY: 0.15,
            QualityMetric.SCALABILITY: 0.12,
            QualityMetric.MAINTAINABILITY: 0.13,
            QualityMetric.TESTING: 0.10,
            QualityMetric.DOCUMENTATION: 0.08,
            QualityMetric.PERFORMANCE: 0.07
        }

        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        self.build_data = self._load_json("artifacts/build/codebase_analysis.json")
        self.assemble_data = self._load_json("artifacts/assemble/dependency_graph.json")
        self.api_data = self._load_json("artifacts/assemble/api_analysis.json")

    def _load_json(self, file_path: str) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON"""
        try:
            with open(self.output_dir / file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            self.logger.warning(f"âš ï¸ ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ {file_path}: {e}")
            return {}

    def assess_architecture(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©"""
        score = 5.0  # Ù†Ù‚Ø·Ø© Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        evidence = []
        recommendations = []

        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø£Ù†Ù…Ø§Ø· Ù…Ø¹Ù…Ø§Ø±ÙŠØ© ÙˆØ§Ø¶Ø­Ø©
        if self.assemble_data.get("modules"):
            module_count = len(self.assemble_data["modules"])

            if module_count > 0:
                score += 1.0
                evidence.append(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {module_count} ÙˆØ­Ø¯Ø©")

                # ÙØ­Øµ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¯ÙˆØ±ÙŠØ©
                circular_deps = self.assemble_data.get("circular_dependencies", [])
                if not circular_deps:
                    score += 1.5
                    evidence.append("Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØ¨Ø¹ÙŠØ§Øª Ø¯ÙˆØ±ÙŠØ©")
                else:
                    score -= len(circular_deps) * 0.5
                    evidence.append(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(circular_deps)} ØªØ¨Ø¹ÙŠØ© Ø¯ÙˆØ±ÙŠØ©")
                    recommendations.append("Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¯ÙˆØ±ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ù†ÙŠØ© Ø§Ù„ÙƒÙˆØ¯")

                # ÙØ­Øµ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù€ coupling
                metrics = self.assemble_data.get("metrics", {})
                coupling = metrics.get("average_coupling", 0.5)

                if coupling < 0.3:
                    score += 1.0
                    evidence.append("Ù…Ø³ØªÙˆÙ‰ coupling Ù…Ù†Ø®ÙØ¶ (Ù…Ù…ØªØ§Ø²)")
                elif coupling < 0.6:
                    score += 0.5
                    evidence.append("Ù…Ø³ØªÙˆÙ‰ coupling Ù…ØªÙˆØ³Ø·")
                else:
                    score -= 0.5
                    evidence.append("Ù…Ø³ØªÙˆÙ‰ coupling Ø¹Ø§Ù„ÙŠ")
                    recommendations.append("ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„ÙˆØ­Ø¯Ø§Øª")

        # ÙØ­Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ
        structure = self.build_data.get("directory_structure", {})
        if structure.get("max_depth", 0) > 8:
            score -= 0.5
            evidence.append(f"Ø¹Ù…Ù‚ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø¹Ø§Ù„ÙŠ: {structure['max_depth']}")
            recommendations.append("Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª")

        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ ÙˆØ§Ø¬Ù‡Ø§Øª API Ù…ÙˆØ«Ù‚Ø©
        apis = self.api_data
        if apis.get("openapi_specs") or apis.get("graphql_schemas"):
            score += 1.0
            evidence.append("ØªÙˆØ¬Ø¯ ÙˆØ§Ø¬Ù‡Ø§Øª API Ù…ÙˆØ«Ù‚Ø©")
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© ØªÙˆØ«ÙŠÙ‚ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø§Øª API")

        return ScoreCard(
            metric="Architecture",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.ARCHITECTURE],
            evidence=evidence,
            recommendations=recommendations
        )

    def assess_code_quality(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯"""
        score = 5.0
        evidence = []
        recommendations = []

        # ÙØ­Øµ ØªÙ†ÙˆØ¹ Ø§Ù„Ù„ØºØ§Øª
        line_counts = self.build_data.get("line_counts", {})
        if isinstance(line_counts, dict) and not line_counts.get("error"):
            languages = [lang for lang in line_counts.keys()
                        if lang not in ["header", "SUM"]]

            if len(languages) <= 3:
                score += 1.0
                evidence.append(f"ØªÙ†ÙˆØ¹ Ù…Ø­Ø¯ÙˆØ¯ ÙÙŠ Ø§Ù„Ù„ØºØ§Øª: {len(languages)}")
            else:
                score -= 0.5
                evidence.append(f"ØªÙ†ÙˆØ¹ Ø¹Ø§Ù„ÙŠ ÙÙŠ Ø§Ù„Ù„ØºØ§Øª Ù‚Ø¯ ÙŠØ¹Ù‚Ø¯ Ø§Ù„ØµÙŠØ§Ù†Ø©: {len(languages)}")

        # ÙØ­Øµ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
        if "header" in line_counts:
            total_lines = line_counts.get("SUM", {}).get("code", 0)
            comment_lines = line_counts.get("SUM", {}).get("comment", 0)

            if total_lines > 0:
                comment_ratio = comment_lines / total_lines
                if 0.1 <= comment_ratio <= 0.3:
                    score += 1.0
                    evidence.append(f"Ù†Ø³Ø¨Ø© ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø©: {comment_ratio:.1%}")
                elif comment_ratio < 0.1:
                    score -= 0.5
                    evidence.append(f"Ù†Ø³Ø¨Ø© ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…Ù†Ø®ÙØ¶Ø©: {comment_ratio:.1%}")
                    recommendations.append("Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ©")

        # ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        structure = self.build_data.get("directory_structure", {})
        large_files = structure.get("large_files", [])

        if not large_files:
            score += 0.5
            evidence.append("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© (> 1MB)")
        else:
            score -= len(large_files) * 0.2
            evidence.append(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©: {len(large_files)}")
            recommendations.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙˆØªØ­Ø³ÙŠÙ†Ù‡Ø§")

        # ÙØ­Øµ Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
        extensions = self.build_data.get("file_extensions", {})
        if extensions:
            # Ù†Ø³Ø¨Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆØ¯ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
            code_extensions = {'.py', '.js', '.ts', '.java', '.go', '.rs', '.cpp', '.c'}
            code_files = sum(count for ext, count in extensions.items()
                           if ext.lower() in code_extensions)
            total_files = sum(extensions.values())

            if total_files > 0:
                code_ratio = code_files / total_files
                if code_ratio > 0.6:
                    score += 0.5
                    evidence.append(f"Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆØ¯: {code_ratio:.1%}")

        return ScoreCard(
            metric="Code Quality",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.CODE_QUALITY],
            evidence=evidence,
            recommendations=recommendations
        )

    def assess_security(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ù…Ø§Ù†"""
        score = 5.0
        evidence = []
        recommendations = []

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø­Ø³Ø§Ø³Ø©
        sensitive_files = [
            ".env", "config.py", "settings.py", "application.properties",
            "config.json", "secrets.yaml", "credentials.json"
        ]

        found_sensitive = []
        for file_pattern in sensitive_files:
            matches = list(self.repo_path.rglob(file_pattern))
            if matches:
                found_sensitive.extend([str(f.name) for f in matches])

        if found_sensitive:
            score -= 1.0
            evidence.append(f"Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ† Ø­Ø³Ø§Ø³Ø©: {', '.join(found_sensitive)}")
            recommendations.append("Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ØªØ¶Ù…ÙŠÙ† Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø­Ø³Ø§Ø³Ø©")
        else:
            score += 0.5
            evidence.append("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ† Ø­Ø³Ø§Ø³Ø© Ø¸Ø§Ù‡Ø±Ø©")

        # ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… HTTPS
        has_https_config = any(
            list(self.repo_path.rglob("*ssl*")) +
            list(self.repo_path.rglob("*tls*")) +
            list(self.repo_path.rglob("*https*"))
        )

        if has_https_config:
            score += 1.0
            evidence.append("ØªÙƒÙˆÙŠÙ† HTTPS/TLS Ù…ÙˆØ¬ÙˆØ¯")
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© ØªÙƒÙˆÙŠÙ† HTTPS/TLS")

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Docker
        dockerfiles = list(self.repo_path.rglob("Dockerfile*"))
        if dockerfiles:
            score += 0.5
            evidence.append("Ø§Ø³ØªØ®Ø¯Ø§Ù… Docker (Ø¹Ø²Ù„ Ø£ÙØ¶Ù„)")

            # ÙØ­Øµ Ø£ÙØ¶Ù„ Ù…Ù…Ø§Ø±Ø³Ø§Øª Docker
            for dockerfile in dockerfiles:
                try:
                    with open(dockerfile, 'r') as f:
                        content = f.read().upper()
                        if "USER" in content and "USER ROOT" not in content:
                            score += 0.5
                            evidence.append("Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± root ÙÙŠ Docker")
                        else:
                            recommendations.append("Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø³ØªØ®Ø¯Ù… ØºÙŠØ± root ÙÙŠ Docker")
                except Exception:
                    pass

        # ÙØ­Øµ Ù†Ù…Ø· ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ÙˆØ± ÙÙŠ Ø§Ù„ÙƒÙˆØ¯
        password_patterns = [
            "password", "passwd", "pwd", "secret", "key", "token"
        ]

        potential_secrets = []
        for code_file in self.repo_path.rglob("*"):
            if code_file.suffix in ['.py', '.js', '.java', '.go', '.rs']:
                try:
                    with open(code_file, 'r', encoding='utf-8') as f:
                        content = f.read().lower()
                        for pattern in password_patterns:
                            if f"{pattern}=" in content or f'"{pattern}"' in content:
                                potential_secrets.append(str(code_file.name))
                                break
                except Exception:
                    continue

        if potential_secrets:
            score -= 0.5
            evidence.append(f"Ù…Ù„ÙØ§Øª Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø³Ø±Ø§Ø±: {len(potential_secrets)}")
            recommendations.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£Ø³Ø±Ø§Ø± Ù…Ø¯Ù…Ø¬Ø©")

        return ScoreCard(
            metric="Security",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.SECURITY],
            evidence=evidence,
            recommendations=recommendations
        )

    def assess_scalability(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹"""
        score = 5.0
        evidence = []
        recommendations = []

        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ containerization
        has_docker = bool(list(self.repo_path.rglob("Dockerfile*")))
        has_compose = bool(list(self.repo_path.rglob("docker-compose*.yml")))
        has_k8s = bool(list(self.repo_path.rglob("k8s/*.yaml"))) or \
                  bool(list(self.repo_path.rglob("kubernetes/*.yaml")))

        container_score = 0
        if has_docker:
            container_score += 1
            evidence.append("Docker containerization")
        if has_compose:
            container_score += 0.5
            evidence.append("Docker Compose Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©")
        if has_k8s:
            container_score += 1.5
            evidence.append("Kubernetes orchestration")

        score += container_score

        if not any([has_docker, has_compose, has_k8s]):
            recommendations.append("Ø¥Ø¶Ø§ÙØ© containerization Ù„ØªØ­Ø³ÙŠÙ† Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹")

        # ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ù„Ù„ØªÙˆØ³Ø¹
        microservices_indicators = [
            "service", "microservice", "api-gateway", "load-balancer"
        ]

        microservices_score = 0
        for indicator in microservices_indicators:
            if any(indicator in str(path).lower() for path in self.repo_path.rglob("*")):
                microservices_score += 0.25

        score += microservices_score
        if microservices_score > 0:
            evidence.append("Ù…Ø¤Ø´Ø±Ø§Øª Ø¹Ù„Ù‰ Ù…Ø¹Ù…Ø§Ø±ÙŠØ© microservices")

        # ÙØ­Øµ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        db_files = (
            list(self.repo_path.rglob("*redis*")) +
            list(self.repo_path.rglob("*mongo*")) +
            list(self.repo_path.rglob("*postgres*")) +
            list(self.repo_path.rglob("*mysql*")) +
            list(self.repo_path.rglob("*elastic*"))
        )

        if db_files:
            score += 0.5
            evidence.append("ØªÙƒÙˆÙŠÙ† Ù‚ÙˆØ§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹")

        # ÙØ­Øµ CI/CD
        ci_files = (
            list(self.repo_path.rglob(".github/workflows/*.yml")) +
            list(self.repo_path.rglob(".gitlab-ci.yml")) +
            list(self.repo_path.rglob("Jenkinsfile")) +
            list(self.repo_path.rglob(".circleci/config.yml"))
        )

        if ci_files:
            score += 1.0
            evidence.append("CI/CD pipeline Ù…ÙˆØ¬ÙˆØ¯")
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© CI/CD pipeline")

        return ScoreCard(
            metric="Scalability",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.SCALABILITY],
            evidence=evidence,
            recommendations=recommendations
        )

    def assess_maintainability(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØµÙŠØ§Ù†Ø©"""
        score = 5.0
        evidence = []
        recommendations = []

        # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ README
        readme_files = list(self.repo_path.rglob("README*"))
        if readme_files:
            score += 1.0
            evidence.append("Ù…Ù„Ù README Ù…ÙˆØ¬ÙˆØ¯")

            # ÙØ­Øµ Ø¬ÙˆØ¯Ø© README
            try:
                readme_content = ""
                for readme in readme_files:
                    with open(readme, 'r', encoding='utf-8') as f:
                        readme_content += f.read()

                if len(readme_content) > 500:
                    score += 0.5
                    evidence.append("README Ù…ÙØµÙ„")

                keywords = ["install", "usage", "example", "api", "contribution"]
                found_keywords = sum(1 for kw in keywords if kw.lower() in readme_content.lower())

                if found_keywords >= 3:
                    score += 0.5
                    evidence.append("README Ø´Ø§Ù…Ù„")

            except Exception:
                pass
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ù README Ø´Ø§Ù…Ù„")

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙˆØ«ÙŠÙ‚
        doc_files = (
            list(self.repo_path.rglob("docs/*")) +
            list(self.repo_path.rglob("documentation/*")) +
            list(self.repo_path.rglob("*.md"))
        )

        if len(doc_files) > 3:
            score += 1.0
            evidence.append(f"ØªÙˆØ«ÙŠÙ‚ Ø´Ø§Ù…Ù„: {len(doc_files)} Ù…Ù„Ù")
        elif len(doc_files) > 0:
            score += 0.5
            evidence.append(f"ØªÙˆØ«ÙŠÙ‚ Ø£Ø³Ø§Ø³ÙŠ: {len(doc_files)} Ù…Ù„Ù")
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙˆØ«ÙŠÙ‚")

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù„Ù„ØªØ·ÙˆÙŠØ±
        dev_files = [
            ".gitignore", ".editorconfig", "package.json", "requirements.txt",
            "setup.py", "Makefile", "pyproject.toml", "composer.json"
        ]

        found_dev_files = []
        for dev_file in dev_files:
            if (self.repo_path / dev_file).exists():
                found_dev_files.append(dev_file)

        dev_score = min(len(found_dev_files) * 0.2, 1.0)
        score += dev_score
        evidence.append(f"Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ† Ø§Ù„ØªØ·ÙˆÙŠØ±: {', '.join(found_dev_files)}")

        # ÙØ­Øµ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        test_files = (
            list(self.repo_path.rglob("test_*.py")) +
            list(self.repo_path.rglob("*test.py")) +
            list(self.repo_path.rglob("*.test.js")) +
            list(self.repo_path.rglob("*Test.java")) +
            list(self.repo_path.rglob("tests/*"))
        )

        if test_files:
            test_score = min(len(test_files) * 0.1, 1.5)
            score += test_score
            evidence.append(f"Ù…Ù„ÙØ§Øª Ø§Ø®ØªØ¨Ø§Ø±: {len(test_files)}")
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ù„ÙƒÙˆØ¯")

        return ScoreCard(
            metric="Maintainability",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.MAINTAINABILITY],
            evidence=evidence,
            recommendations=recommendations
        )

    def assess_testing(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        score = 3.0  # Ù†Ù‚Ø·Ø© Ø¨Ø¯Ø§ÙŠØ© Ø£Ù‚Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        evidence = []
        recommendations = []

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        test_patterns = [
            "test_*.py", "*_test.py", "*.test.js", "*.spec.js",
            "*Test.java", "*Tests.java", "*_test.go", "*test.php"
        ]

        all_test_files = []
        for pattern in test_patterns:
            all_test_files.extend(list(self.repo_path.rglob(pattern)))

        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        all_code_files = []
        code_patterns = ["*.py", "*.js", "*.java", "*.go", "*.php", "*.ts"]
        for pattern in code_patterns:
            all_code_files.extend(list(self.repo_path.rglob(pattern)))

        # ØªØµÙÙŠØ© Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆØ¯
        code_files = [f for f in all_code_files if f not in all_test_files]

        if code_files and all_test_files:
            test_ratio = len(all_test_files) / len(code_files)

            if test_ratio >= 0.8:
                score += 4.0
                evidence.append(f"ØªØºØ·ÙŠØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ù…ØªØ§Ø²Ø©: {test_ratio:.1%}")
            elif test_ratio >= 0.5:
                score += 3.0
                evidence.append(f"ØªØºØ·ÙŠØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¬ÙŠØ¯Ø©: {test_ratio:.1%}")
            elif test_ratio >= 0.2:
                score += 1.5
                evidence.append(f"ØªØºØ·ÙŠØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©: {test_ratio:.1%}")
            else:
                score += 0.5
                evidence.append(f"ØªØºØ·ÙŠØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ù†Ø®ÙØ¶Ø©: {test_ratio:.1%}")
                recommendations.append("Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª")

        elif all_test_files:
            score += 2.0
            evidence.append(f"ÙŠÙˆØ¬Ø¯ {len(all_test_files)} Ù…Ù„Ù Ø§Ø®ØªØ¨Ø§Ø±")
        else:
            evidence.append("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª")
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø©")

        # ÙØ­Øµ Ø£Ø·Ø± Ø¹Ù…Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        test_frameworks = {
            "pytest": ["pytest", "conftest.py"],
            "unittest": ["unittest"],
            "jest": ["jest.config", "package.json"],
            "mocha": ["mocha", ".mocharc"],
            "junit": ["junit", "pom.xml"],
            "cypress": ["cypress.json", "cypress/"],
            "selenium": ["selenium"]
        }

        found_frameworks = []
        for framework, indicators in test_frameworks.items():
            for indicator in indicators:
                if list(self.repo_path.rglob(f"*{indicator}*")):
                    found_frameworks.append(framework)
                    break

        if found_frameworks:
            score += len(found_frameworks) * 0.5
            evidence.append(f"Ø£Ø·Ø± Ø§Ø®ØªØ¨Ø§Ø±: {', '.join(found_frameworks)}")

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ† Ø§Ù„ØªØºØ·ÙŠØ©
        coverage_files = list(self.repo_path.rglob(".coveragerc")) + \
                        list(self.repo_path.rglob("coverage.xml")) + \
                        list(self.repo_path.rglob("jest.config.js"))

        if coverage_files:
            score += 0.5
            evidence.append("ØªÙƒÙˆÙŠÙ† Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØºØ·ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯")

        return ScoreCard(
            metric="Testing",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.TESTING],
            evidence=evidence,
            recommendations=recommendations
        )

    def assess_documentation(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªÙˆØ«ÙŠÙ‚"""
        score = 3.0
        evidence = []
        recommendations = []

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        essential_docs = [
            ("README", ["README.md", "README.rst", "README.txt"]),
            ("CHANGELOG", ["CHANGELOG.md", "CHANGES.md", "HISTORY.md"]),
            ("LICENSE", ["LICENSE", "LICENSE.md", "LICENSE.txt"]),
            ("CONTRIBUTING", ["CONTRIBUTING.md", "CONTRIBUTING.rst"]),
            ("API Documentation", ["docs/api", "api.md", "API.md"])
        ]

        for doc_type, file_patterns in essential_docs:
            found = False
            for pattern in file_patterns:
                if list(self.repo_path.rglob(pattern)):
                    found = True
                    break

            if found:
                score += 1.0
                evidence.append(f"{doc_type} Ù…ÙˆØ¬ÙˆØ¯")
            else:
                recommendations.append(f"Ø¥Ø¶Ø§ÙØ© {doc_type}")

        # ÙØ­Øµ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù…Ø®ØµØµ
        docs_dirs = list(self.repo_path.rglob("docs/")) + \
                   list(self.repo_path.rglob("documentation/"))

        if docs_dirs:
            docs_files = []
            for docs_dir in docs_dirs:
                docs_files.extend(list(docs_dir.rglob("*.md")))
                docs_files.extend(list(docs_dir.rglob("*.rst")))

            if len(docs_files) >= 5:
                score += 1.5
                evidence.append(f"ØªÙˆØ«ÙŠÙ‚ Ø´Ø§Ù…Ù„: {len(docs_files)} Ù…Ù„Ù")
            elif len(docs_files) > 0:
                score += 0.5
                evidence.append(f"ØªÙˆØ«ÙŠÙ‚ Ø£Ø³Ø§Ø³ÙŠ: {len(docs_files)} Ù…Ù„Ù")

        # ÙØ­Øµ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ÙƒÙˆØ¯ (docstrings/comments)
        documented_files = 0
        total_code_files = 0

        for code_file in self.repo_path.rglob("*.py"):
            if any(part.startswith('.') for part in code_file.parts):
                continue

            total_code_files += 1
            try:
                with open(code_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if '"""' in content or "'''" in content or "def " in content and "#" in content:
                        documented_files += 1
            except Exception:
                continue

        if total_code_files > 0:
            doc_ratio = documented_files / total_code_files
            if doc_ratio >= 0.7:
                score += 1.0
                evidence.append(f"Ù†Ø³Ø¨Ø© ØªÙˆØ«ÙŠÙ‚ Ø§Ù„ÙƒÙˆØ¯ Ø¹Ø§Ù„ÙŠØ©: {doc_ratio:.1%}")
            elif doc_ratio >= 0.3:
                score += 0.5
                evidence.append(f"Ù†Ø³Ø¨Ø© ØªÙˆØ«ÙŠÙ‚ Ø§Ù„ÙƒÙˆØ¯ Ù…ØªÙˆØ³Ø·Ø©: {doc_ratio:.1%}")
            else:
                recommendations.append("Ø²ÙŠØ§Ø¯Ø© ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ùˆtocstrings ÙÙŠ Ø§Ù„ÙƒÙˆØ¯")

        return ScoreCard(
            metric="Documentation",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.DOCUMENTATION],
            evidence=evidence,
            recommendations=recommendations
        )

    def assess_performance(self) -> ScoreCard:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        score = 5.0
        evidence = []
        recommendations = []

        # ÙØ­Øµ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        optimization_indicators = {
            "caching": ["cache", "redis", "memcached"],
            "database_optimization": ["index", "query_optimization", "orm"],
            "cdn": ["cdn", "cloudflare", "fastly"],
            "compression": ["gzip", "compression", "minify"],
            "async": ["async", "await", "promise", "concurrent"]
        }

        found_optimizations = []
        for opt_type, keywords in optimization_indicators.items():
            for keyword in keywords:
                if any(keyword in str(path).lower() for path in self.repo_path.rglob("*")):
                    found_optimizations.append(opt_type)
                    break

        if found_optimizations:
            score += len(found_optimizations) * 0.5
            evidence.append(f"ØªØ­Ø³ÙŠÙ†Ø§Øª Ø£Ø¯Ø§Ø¡: {', '.join(found_optimizations)}")

        # ÙØ­Øµ Ù…Ù„ÙØ§Øª Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡
        benchmark_files = (
            list(self.repo_path.rglob("*benchmark*")) +
            list(self.repo_path.rglob("*performance*")) +
            list(self.repo_path.rglob("*profiling*"))
        )

        if benchmark_files:
            score += 1.0
            evidence.append("Ù…Ù„ÙØ§Øª Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…ÙˆØ¬ÙˆØ¯Ø©")
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡")

        # ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
        monitoring_tools = [
            "prometheus", "grafana", "newrelic", "datadog",
            "sentry", "bugsnag", "rollbar"
        ]

        found_monitoring = []
        for tool in monitoring_tools:
            if any(tool in str(path).lower() for path in self.repo_path.rglob("*")):
                found_monitoring.append(tool)

        if found_monitoring:
            score += 1.0
            evidence.append(f"Ø£Ø¯ÙˆØ§Øª Ù…Ø±Ø§Ù‚Ø¨Ø©: {', '.join(found_monitoring)}")
        else:
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ø£Ø¯ÙˆØ§Øª Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡")

        return ScoreCard(
            metric="Performance",
            score=min(score, 10.0),
            weight=self.weights[QualityMetric.PERFORMANCE],
            evidence=evidence,
            recommendations=recommendations
        )

    def generate_risk_register(self, scorecards: List[ScoreCard]) -> List[RiskItem]:
        """Ø¥Ù†ØªØ§Ø¬ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±"""
        risks = []

        # Ù…Ø®Ø§Ø·Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
        for scorecard in scorecards:
            if scorecard.score < 6.0:
                risk_level = RiskLevel.HIGH if scorecard.score < 4.0 else RiskLevel.MEDIUM

                risks.append(RiskItem(
                    id=f"RISK-{scorecard.metric.upper()}-001",
                    title=f"Ø¯Ø±Ø¬Ø© Ù…Ù†Ø®ÙØ¶Ø© ÙÙŠ {scorecard.metric}",
                    description=f"Ø­ØµÙ„ Ù…Ø¹ÙŠØ§Ø± {scorecard.metric} Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø© {scorecard.score:.1f}/10",
                    category="quality",
                    probability=0.8,
                    impact=scorecard.weight * 10,
                    level=risk_level,
                    mitigation_strategies=scorecard.recommendations
                ))

        # Ù…Ø®Ø§Ø·Ø± Ù…Ø­Ø¯Ø¯Ø©

        # Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø£Ù…Ø§Ù†
        security_score = next((s.score for s in scorecards if s.metric == "Security"), 5.0)
        if security_score < 7.0:
            risks.append(RiskItem(
                id="RISK-SECURITY-002",
                title="Ø«ØºØ±Ø§Øª Ø£Ù…Ù†ÙŠØ© Ù…Ø­ØªÙ…Ù„Ø©",
                description="ÙˆØ¬ÙˆØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø¹Ù„Ù‰ Ø¶Ø¹Ù ÙÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ©",
                category="security",
                probability=0.6,
                impact=9.0,
                level=RiskLevel.HIGH,
                mitigation_strategies=[
                    "Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ù…Ù†ÙŠØ© Ø´Ø§Ù…Ù„Ø©",
                    "ØªØ·Ø¨ÙŠÙ‚ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ©",
                    "Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª ÙØ­Øµ Ø§Ù„Ø«ØºØ±Ø§Øª"
                ]
            ))

        # Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¯ÙˆØ±ÙŠØ©
        circular_deps = self.assemble_data.get("circular_dependencies", [])
        if circular_deps:
            risks.append(RiskItem(
                id="RISK-ARCH-001",
                title="ØªØ¨Ø¹ÙŠØ§Øª Ø¯ÙˆØ±ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©",
                description=f"ÙˆØ¬ÙˆØ¯ {len(circular_deps)} ØªØ¨Ø¹ÙŠØ© Ø¯ÙˆØ±ÙŠØ© Ù‚Ø¯ ØªØ¹Ù‚Ø¯ Ø§Ù„ØµÙŠØ§Ù†Ø©",
                category="architecture",
                probability=1.0,
                impact=6.0,
                level=RiskLevel.MEDIUM,
                mitigation_strategies=[
                    "Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„ÙƒÙˆØ¯ Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¯ÙˆØ±ÙŠØ©",
                    "ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¨Ø§Ø¯Ø¦ SOLID",
                    "Ø§Ø³ØªØ®Ø¯Ø§Ù… Dependency Injection"
                ]
            ))

        # Ù…Ø®Ø§Ø·Ø± Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹
        scalability_score = next((s.score for s in scorecards if s.metric == "Scalability"), 5.0)
        if scalability_score < 6.0:
            risks.append(RiskItem(
                id="RISK-SCALE-001",
                title="Ù‚ÙŠÙˆØ¯ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹",
                description="Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù‚Ø¯ ÙŠÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø§Øª ÙÙŠ Ø§Ù„ØªÙˆØ³Ø¹",
                category="scalability",
                probability=0.7,
                impact=7.0,
                level=RiskLevel.MEDIUM,
                mitigation_strategies=[
                    "ØªØ·Ø¨ÙŠÙ‚ containerization",
                    "ØªØµÙ…ÙŠÙ… Ù…Ø¹Ù…Ø§Ø±ÙŠØ© microservices",
                    "Ø§Ø³ØªØ®Ø¯Ø§Ù… load balancing"
                ]
            ))

        return risks

    def calculate_overall_score(self, scorecards: List[ScoreCard]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"""
        total_weighted_score = sum(card.weighted_score for card in scorecards)
        total_weight = sum(card.weight for card in scorecards)

        if total_weight > 0:
            return total_weighted_score / total_weight
        return 0.0

    def generate_scorecard_report(self) -> Dict[str, Any]:
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„"""
        self.logger.info("ğŸ“Š Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")

        # ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª
        scorecards = [
            self.assess_architecture(),
            self.assess_code_quality(),
            self.assess_security(),
            self.assess_scalability(),
            self.assess_maintainability(),
            self.assess_testing(),
            self.assess_documentation(),
            self.assess_performance()
        ]

        # Ø¥Ù†ØªØ§Ø¬ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        risks = self.generate_risk_register(scorecards)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        overall_score = self.calculate_overall_score(scorecards)

        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
        if overall_score >= 8.5:
            grade = "Ù…Ù…ØªØ§Ø²"
            readiness = "Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬"
        elif overall_score >= 7.0:
            grade = "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹"
            readiness = "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø·ÙÙŠÙØ©"
        elif overall_score >= 5.5:
            grade = "Ø¬ÙŠØ¯"
            readiness = "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…ØªÙˆØ³Ø·Ø©"
        elif overall_score >= 4.0:
            grade = "Ù…Ù‚Ø¨ÙˆÙ„"
            readiness = "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†Ø§Øª ÙƒØ¨ÙŠØ±Ø©"
        else:
            grade = "Ø¶Ø¹ÙŠÙ"
            readiness = "ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø´Ø§Ù…Ù„Ø©"

        report = {
            "summary": {
                "overall_score": round(overall_score, 2),
                "grade": grade,
                "readiness": readiness,
                "total_risks": len(risks),
                "high_risks": len([r for r in risks if r.level == RiskLevel.HIGH]),
                "assessment_date": json.dumps(None, default=str)  # Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ù„ÙŠ
            },
            "detailed_scores": [asdict(card) for card in scorecards],
            "risk_register": [asdict(risk) for risk in risks],
            "recommendations": {
                "immediate": [],
                "short_term": [],
                "long_term": []
            }
        }

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        all_recommendations = []
        for card in scorecards:
            all_recommendations.extend(card.recommendations)

        # ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙˆØµÙŠØ§Øª (Ø¨Ø³ÙŠØ· - ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡)
        for rec in all_recommendations:
            if any(word in rec.lower() for word in ["Ø£Ù…Ø§Ù†", "Ø«ØºØ±Ø©", "Ø­Ø±Ø¬"]):
                report["recommendations"]["immediate"].append(rec)
            elif any(word in rec.lower() for word in ["Ø§Ø®ØªØ¨Ø§Ø±", "ØªÙˆØ«ÙŠÙ‚", "ØªØ¹Ù„ÙŠÙ‚"]):
                report["recommendations"]["short_term"].append(rec)
            else:
                report["recommendations"]["long_term"].append(rec)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
        for category in report["recommendations"]:
            report["recommendations"][category] = list(set(report["recommendations"][category]))

        return report

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    import sys
    from datetime import datetime

    if len(sys.argv) < 3:
        print("Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: python grade_assessment.py <repo_path> <output_dir>")
        sys.exit(1)

    repo_path = sys.argv[1]
    output_dir = sys.argv[2]

    assessor = QualityAssessor(repo_path, output_dir)
    report = assessor.generate_scorecard_report()

    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ù„ÙŠ
    report["summary"]["assessment_date"] = datetime.now().isoformat()

    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    output_path = Path(output_dir) / "artifacts/grade/scorecard.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ
    print(f"âœ… ØªÙ… Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…")
    print(f"ğŸ“Š Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {report['summary']['overall_score']}/10")
    print(f"ğŸ† Ø§Ù„ØªØµÙ†ÙŠÙ: {report['summary']['grade']}")
    print(f"âš ï¸ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¹Ø§Ù„ÙŠØ©: {report['summary']['high_risks']}")
    print(f"ğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {output_path}")

if __name__ == "__main__":
    main()